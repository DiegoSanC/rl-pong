{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85646770-4c8a-437b-86f0-fd988bc38e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "from math import ceil,floor\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af9b7518-9078-431d-849d-67a28f49de24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "\n",
    "    def __init__(self, game, policy=None, discount_factor=0.1, learning_rate=0.1, exploitation_rate=0.9):\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        :param game:\n",
    "        :param policy:\n",
    "        :param discount_factor:\n",
    "        :param learning_rate:\n",
    "        :param ratio_explotacion:\n",
    "        \"\"\"\n",
    "\n",
    "        # Build policy table\n",
    "        if policy is not None:\n",
    "            self._q_table = policy\n",
    "        else:\n",
    "            position = list(game.positions_space.shape)\n",
    "            position.append(len(game.action_space))\n",
    "            self._q_table = np.zeros(position)\n",
    "\n",
    "        self.discount_factor = discount_factor\n",
    "        self.learning_rate = learning_rate\n",
    "        self.exploitation_rate = exploitation_rate\n",
    "\n",
    "    def get_next_step(self, state, game):\n",
    "        \"\"\"\n",
    "        Select next best action (max) with the information given by the state. Some times, it generates a random\n",
    "        move (up or down) to explore new possible combinations\n",
    "\n",
    "        :param state: current state (agent_position, ball_x, ball_y)\n",
    "        :param game: environment\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # Random step for exploring\n",
    "        next_step = np.random.choice(list(game.action_space))\n",
    "\n",
    "        # np.random.uniform returns a number between [0,1]\n",
    "        if np.random.uniform() <= self.exploitation_rate:\n",
    "            # in the current state we select the max value between 2 possible actions (Up, Down)\n",
    "            idx_action = np.random.choice(np.flatnonzero(\n",
    "                self._q_table[state[0], state[1], state[2]] == self._q_table[state[0], state[1], state[2]].max()\n",
    "            ))\n",
    "            next_step = list(game.action_space)[idx_action]\n",
    "\n",
    "        return next_step\n",
    "\n",
    "    # We update policy with obtained rewards\n",
    "    def update(self, game, old_state, action_taken, reward_action_taken, new_state, reached_end):\n",
    "        \"\"\"\n",
    "        Computes updates of the policy tables\n",
    "        :param game: environment\n",
    "        :param old_state: last known state before agent action taken\n",
    "        :param action_taken: next action taken by the agent\n",
    "        :param reward_action_taken: reward obtained by the action (10 when colission, -10 when ball hits left wall, 0\n",
    "        otherwise)\n",
    "        :param new_state: current state after agent has taken an action\n",
    "        :param reached_end: if game has ended\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        #we get the index (0 or 1) depending on the action selected in get_next_step\n",
    "        idx_action_taken = list(game.action_space).index(action_taken)\n",
    "\n",
    "\n",
    "        #for agent and ball positions we get 2 possible options (up and down) which initially have two 0s\n",
    "        current_q_value_options = self._q_table[old_state[0], old_state[1], old_state[2]]\n",
    "        #This change the position of the agent so we need the table for that new agent position\n",
    "        current_q_value = current_q_value_options[idx_action_taken]\n",
    "\n",
    "        future_q_value_options = self._q_table[new_state[0], new_state[1], new_state[2]]\n",
    "\n",
    "        # future_max_q_value = R + (lambda * maxQ(s'))\n",
    "        future_max_q_value = reward_action_taken + self.discount_factor * future_q_value_options.max()\n",
    "        if reached_end:\n",
    "            future_max_q_value = reward_action_taken  # maximum reward\n",
    "\n",
    "        # Q^(s,a) = Q(s,a) + alpha*[future_max_q_value - Q(s,a)\n",
    "        self._q_table[old_state[0], old_state[1], old_state[2], idx_action_taken] = current_q_value + \\\n",
    "                                                                                    self.learning_rate * \\\n",
    "                                                                                    (future_max_q_value -\n",
    "                                                                                     current_q_value)\n",
    "\n",
    "    def print_policy(self):\n",
    "        for row in np.round(self._q_table, 1):\n",
    "            for column in row:\n",
    "                print('[', end='')\n",
    "                for value in column:\n",
    "                    print(str(value).zfill(5), end=' ')\n",
    "                print('] ', end='')\n",
    "            print('')\n",
    "\n",
    "    def get_policy(self):\n",
    "        return self._q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae9f4b34-019c-4db4-a96f-a4cbcf43fc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, max_life=3, height_px=40, width_px=50, mov_px=3):\n",
    "        \"\"\"\n",
    "\n",
    "        :param max_life: num max lives\n",
    "        :param height_px: height of environment matrix\n",
    "        :param width_px: width of environment matrix\n",
    "        :param mov_px: move of agent and ball in pixels\n",
    "        \"\"\"\n",
    "\n",
    "        self.action_space = ['Up', 'Down']\n",
    "\n",
    "        self._step_penalization = 0\n",
    "\n",
    "        #The agente only makes movs in vertical axys. In each of those vertical positions,\n",
    "        # there is going to be a one 2D table for the ball position\n",
    "        self.state = [0, 0, 0]\n",
    "\n",
    "        self.total_reward = 0\n",
    "\n",
    "        # 3pixels movements for the ball and the agent\n",
    "        self.dx = mov_px\n",
    "        self.dy = mov_px\n",
    "\n",
    "        rows = math.ceil(height_px / mov_px)\n",
    "        columns = math.ceil(width_px / mov_px)\n",
    "\n",
    "\n",
    "        self.positions_space = np.array([[[0 for z in range(columns)]\n",
    "                                          for y in range(rows)]\n",
    "                                         for x in range(rows)])\n",
    "\n",
    "        self.lives = max_life\n",
    "        self.max_life = max_life\n",
    "\n",
    "        self.x_ball = random.randint(int(width_px / 2), width_px)\n",
    "        self.y_ball = random.randint(0, height_px - 10)\n",
    "\n",
    "        self.player_height = int(height_px / 4)\n",
    "\n",
    "        self.player_1 = self.player_height  # posic. inicial del player\n",
    "\n",
    "        self.score = 0\n",
    "\n",
    "        self.width_px = width_px\n",
    "        self.height_px = height_px\n",
    "        self.radius = 2.5\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset environment\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.total_reward = 0\n",
    "        self.state = [0, 0, 0]\n",
    "        self.lives = self.max_life\n",
    "        self.score = 0\n",
    "        self.x_ball = random.randint(int(self.width_px / 2), self.width_px)\n",
    "        self.y_ball = random.randint(0, self.height_px - 10)\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action, animate=False):\n",
    "        \"\"\"\n",
    "        Execute apply_action, check if agent has lost all lives, computes reward as sum of score and step_penalization\n",
    "        and updates total_reward with new calculated reward\n",
    "        :param action:\n",
    "        :param animate:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.apply_action(action, animate)\n",
    "        done = self.lives <= 0\n",
    "        reward = self.score\n",
    "        reward += self._step_penalization\n",
    "        self.total_reward += reward\n",
    "        return self.state, reward, done\n",
    "\n",
    "    def apply_action(self, action, animate=False):\n",
    "        \"\"\"\n",
    "        Apply one of two possible actions comming from the agent. In this case, actions can be Up an Down, and\n",
    "        both are represented as a movement of x num of pixels in the board (negative or positive)\n",
    "        :param action:\n",
    "        :param animate:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if action == \"Up\":\n",
    "            self.player_1 += abs(self.dy)\n",
    "        elif action == \"Down\":\n",
    "            self.player_1 -= abs(self.dy)\n",
    "\n",
    "        #We represent the movement of th agent\n",
    "        self.step_player()\n",
    "\n",
    "        #We represent the movement of the ball and the possible collision that will determine\n",
    "        # if the agent gets a reward or a punishment\n",
    "        self.step_ball()\n",
    "\n",
    "        if animate:\n",
    "            clear_output(wait=True)\n",
    "            fig = self.draw_frame()\n",
    "            plt.show()\n",
    "\n",
    "        #We set new state after agent and ball have moved\n",
    "        self.state = [math.floor(self.player_1 / abs(self.dy)) - 2, math.floor(self.y_ball / abs(self.dy)) - 2,\n",
    "                      math.floor(self.x_ball / abs(self.dx)) - 2]\n",
    "\n",
    "    def detect_collision(self, ball_y, player_y):\n",
    "        \"\"\"\n",
    "        Detects if there is a collision between agent and ball. This method is called if in x coor agent and ball\n",
    "        are close enough\n",
    "        :param ball_y: y coor of ball\n",
    "        :param player_y: y coor agent\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        #If the top of the player is higher than y-bottom of the ball is a collision.\n",
    "        # If bottom of the agent is lower than y-top of the ball, then whe have collision.\n",
    "        # when ball hits in any other point of the agent, both conditions are going to be matched\n",
    "        if (player_y+self.player_height >= (ball_y-self.radius)) and (player_y <= (ball_y+self.radius)):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def step_player(self):\n",
    "        \"\"\"\n",
    "        computes an agent step\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        #Upper limit\n",
    "        if self.player_1 + self.player_height >= self.height_px:\n",
    "            self.player_1 = self.height_px - self.player_height\n",
    "        elif self.player_1 <= -abs(self.dy):\n",
    "            self.player_1 = -abs(self.dy)\n",
    "\n",
    "    def step_ball(self):\n",
    "        \"\"\"\n",
    "        Computes a ball step in the matrix\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.x_ball += self.dx\n",
    "        self.y_ball += self.dy\n",
    "\n",
    "        #the ball bounces so the sense of the x movements is inverted\n",
    "        if self.x_ball <= 3 or self.x_ball > self.width_px:\n",
    "            self.dx = -self.dx\n",
    "\n",
    "            #As the radius is 2.5 we have to evaluate if there is a collision\n",
    "            if self.x_ball <= 3:\n",
    "                ret = self.detect_collision(self.y_ball, self.player_1)\n",
    "\n",
    "                if ret:\n",
    "                    self.score = 10\n",
    "                else:\n",
    "                    self.score = -10\n",
    "                    self.lives -= 1\n",
    "                    if self.lives > 0:\n",
    "                        self.x_ball = random.randint(int(self.width_px / 2), self.width_px)\n",
    "                        self.y_ball = random.randint(0, self.height_px - 10)\n",
    "                        self.dx = abs(self.dx)\n",
    "                        self.dy = abs(self.dy)\n",
    "        else:\n",
    "            self.score = 0\n",
    "\n",
    "        # the ball bounces so the sense of the y movements is inverted\n",
    "        if self.y_ball < 0 or self.y_ball > self.height_px:\n",
    "            self.dy = -self.dy\n",
    "\n",
    "    def draw_frame(self):\n",
    "        \"\"\"\n",
    "        For drawing environment and movements of agent a ball\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        fig = plt.figure(figsize=(5, 4))\n",
    "        a1 = plt.gca()\n",
    "        circle = plt.Circle((self.x_ball, self.y_ball), self.radius, fc='slategray', ec=\"black\")\n",
    "        a1.set_ylim(-5, self.height_px + 5)\n",
    "        a1.set_xlim(-5, self.width_px + 5)\n",
    "\n",
    "        rectangle = plt.Rectangle((-5, self.player_1), 5, self.player_height, fc='gold', ec=\"none\")\n",
    "        a1.add_patch(circle)\n",
    "        a1.add_patch(rectangle)\n",
    "        # a1.set_yticklabels([]);a1.set_xticklabels([]);\n",
    "        plt.text(4, self.height_px, \"SCORE:\" + str(self.total_reward) + \"  LIFE:\" + str(self.lives), fontsize=12)\n",
    "        if self.lives <=0:\n",
    "            plt.text(10, self.height_px - 14, \"GAME OVER\", fontsize=16)\n",
    "        elif self.total_reward >= 1000:\n",
    "            plt.text(10, self.height_px - 14, \"YOU WIN!\", fontsize=16)\n",
    "        return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a848cd9b-446b-41e6-bbf1-f79ad17a7417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(rounds=5000, max_life=3, discount_factor = 0.1, learning_rate = 0.1,\n",
    "         exploitation_rate=0.9,learner=None, game=None, animate=False):\n",
    "\n",
    "    if game is None:\n",
    "        # If we use mov_px = 5 => policy tables will be 8x10\n",
    "        # If we use mov_px = 3 => policy tables will be 14x17\n",
    "        game = Environment(max_life=max_life, mov_px=3)\n",
    "        \n",
    "    if learner is None:\n",
    "        print(\"Begin new Train!\")\n",
    "        learner = Agent(game, discount_factor=discount_factor,learning_rate=learning_rate,\n",
    "                            exploitation_rate=exploitation_rate)\n",
    "\n",
    "    max_points= -9999\n",
    "    first_max_reached = 0\n",
    "    total_rw=0\n",
    "    steps=[]\n",
    "\n",
    "    for played_games in range(0, rounds):\n",
    "        state = game.reset()\n",
    "        reward, done = None, None\n",
    "        \n",
    "        itera=0\n",
    "        while (done != True) and (itera < 3000 and game.total_reward<=1000):\n",
    "            old_state = np.array(state)\n",
    "            next_action = learner.get_next_step(state, game)\n",
    "            state, reward, done = game.step(next_action, animate=animate)\n",
    "            if rounds > 1:\n",
    "                learner.update(game, old_state, next_action, reward, state, done)\n",
    "            itera+=1\n",
    "        \n",
    "        steps.append(itera)\n",
    "        \n",
    "        total_rw+=game.total_reward\n",
    "        if game.total_reward > max_points:\n",
    "            max_points=game.total_reward\n",
    "            first_max_reached = played_games\n",
    "        \n",
    "        if played_games %500==0 and played_games >1 and not animate:\n",
    "            print(\"-- Games[\", played_games, \"] Avg.Points[\", int(total_rw/played_games),\"]  AVG Steps[\", int(np.array(steps).mean()), \"] Max Score[\", max_points,\"]\")\n",
    "                \n",
    "    if played_games>1:\n",
    "        print('Games[',played_games,'] Avg.Points[',int(total_rw/played_games),'] Max score[', max_points,'] in game [',first_max_reached,']')\n",
    "        \n",
    "    return learner, game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01737d4d-00a2-45c0-bce8-fd6394fbe8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin new Train!\n",
      "-- Games[ 500 ] Avg.Points[ 18 ]  AVG Steps[ 234 ] Max Score[ 120 ]\n",
      "-- Games[ 1000 ] Avg.Points[ 25 ]  AVG Steps[ 255 ] Max Score[ 130 ]\n",
      "-- Games[ 1500 ] Avg.Points[ 27 ]  AVG Steps[ 262 ] Max Score[ 280 ]\n",
      "-- Games[ 2000 ] Avg.Points[ 30 ]  AVG Steps[ 273 ] Max Score[ 310 ]\n",
      "-- Games[ 2500 ] Avg.Points[ 32 ]  AVG Steps[ 279 ] Max Score[ 310 ]\n",
      "-- Games[ 3000 ] Avg.Points[ 34 ]  AVG Steps[ 286 ] Max Score[ 310 ]\n",
      "-- Games[ 3500 ] Avg.Points[ 36 ]  AVG Steps[ 292 ] Max Score[ 310 ]\n",
      "-- Games[ 4000 ] Avg.Points[ 38 ]  AVG Steps[ 300 ] Max Score[ 430 ]\n",
      "-- Games[ 4500 ] Avg.Points[ 40 ]  AVG Steps[ 306 ] Max Score[ 430 ]\n",
      "Games[ 4999 ] Avg.Points[ 41 ] Max score[ 480 ] in game [ 4711 ]\n"
     ]
    }
   ],
   "source": [
    "learner, game = play(rounds=5000, discount_factor = 0.2, learning_rate = 0.1, exploitation_rate=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "773522f0-54c3-42b8-99ca-98361cc35fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learner2 = Agent(game, policy=learner.get_policy())\n",
    "learner2.exploitation_rate = 1.0  # Not random actions while playing\n",
    "player = play(rounds=1, learner=learner2, game=game, animate=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
